# rtc_integration_layer.py
import asyncio
import os
from pathlib import Path
from typing import AsyncGenerator, Callable, Optional, Tuple, Any, Dict, ClassVar
from datetime import datetime as _dt
import numpy as np 
import torch 
import traceback 
import time 
import base64
import httpx 
import whisper # C·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán Whisper
from concurrent.futures import ThreadPoolExecutor

# --- C·∫•u h√¨nh API N·ªôi b·ªô ---
INTERNAL_UPLOAD_URL = "http://internal.company.api/v1/voice_logs/upload" 
INTERNAL_API_KEY = "YOUR_DEFAULT_INTERNAL_API_KEY_HERE" 
# ----------------------------------------------------------------------

# --- SAFE IMPORTS (CONFIG, DIALOG MANAGER V√Ä RESPONSE GENERATOR) ---
try:
    from config_db import WHISPER_MODEL_NAME, SAMPLE_RATE 
    from dialog_manager import DialogManager 
    from response_generator import ResponseGenerator # ResponseGenerator ƒë∆∞·ª£c DialogManager s·ª≠ d·ª•ng
except ImportError:
    # Fallback/Mock n·∫øu kh√¥ng t√¨m th·∫•y c√°c l·ªõp c·ªët l√µi
    WHISPER_MODEL_NAME = "tiny" 
    SAMPLE_RATE = 16000
    class ResponseGenerator:
        def __init__(self, *args, **kwargs): pass
    class DialogManager:
        def __init__(self, *args, **kwargs): pass
        def process_audio_file(self, record_file, user_input_asr): 
            res_text = f"L·ªñI DM: Kh√¥ng t√¨m th·∫•y DialogManager. ASR: {user_input_asr}"
            if "[NO SPEECH DETECTED]" in user_input_asr:
                 res_text = "Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i kh√¥ng?"
            return {"response_text": res_text, "response_audio_path": None, "user_input_asr": user_input_asr}


RECORDING_DIR = Path("rtc_recordings"); RECORDING_DIR.mkdir(exist_ok=True) 

def _log_colored(message, color="white"):
    color_map = {
        "red": "\033[91m", "green": "\033[92m", "yellow": "\033[93m", 
        "blue": "\033[94m", "magenta": "\033[95m", "cyan": "\033[96m", "white": "\033[97m", "orange": "\033[33m"
    }
    RESET = "\033[0m"
    print(f"{color_map.get(color, RESET)}{message}{RESET}", flush=True)


# ==================== VAD/ASR LOGIC ====================
WHISPER_IS_READY = False
try:
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_FP16 = (DEVICE == "cuda") 
    
    # T·∫£i VAD (Silero)
    VAD_MODEL, VAD_UTILS = torch.hub.load(
        repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=False, trust_repo=True 
    )
    VAD_MODEL = VAD_MODEL.to(DEVICE)
    (get_speech_timestamps, save_audio, read_audio, VAD_collect_chunks, *vad_extra_utils) = VAD_UTILS 
    
    # T·∫£i Whisper
    WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME)
    if USE_FP16: WHISPER_MODEL = WHISPER_MODEL.half()
    WHISPER_MODEL.to(DEVICE)
    WHISPER_IS_READY = True
except Exception as e:
    DEVICE = "cpu"
    WHISPER_MODEL = None
    _log_colored(f"‚ùå L·ªói kh·ªüi t·∫°o ASR/VAD (Whisper/Torch): {e}", "red")

def _apply_silero_vad(audio_filepath: Path, log_callback: Callable) -> Optional[np.ndarray]:
    """√Åp d·ª•ng VAD ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng l·∫∑ng."""
    if not WHISPER_IS_READY: return None
    try:
        audio_numpy = whisper.load_audio(str(audio_filepath))
        audio_tensor = torch.from_numpy(audio_numpy).float()
        speech_timestamps = get_speech_timestamps(audio_tensor.to(DEVICE), VAD_MODEL, sampling_rate=SAMPLE_RATE, threshold=0.3)
        if not speech_timestamps: return None 
        speech_audio_tensor = VAD_collect_chunks(speech_timestamps, audio_tensor)
        speech_audio_numpy = speech_audio_tensor.cpu().numpy()
        MIN_SPEECH_DURATION_SECONDS = 0.5
        filtered_duration = len(speech_audio_numpy) / SAMPLE_RATE
        if filtered_duration < MIN_SPEECH_DURATION_SECONDS: return None 
        return speech_audio_numpy
    except Exception:
        return whisper.load_audio(str(audio_filepath))

class ASRServiceWhisper:
    def __init__(self, log_callback: Callable, model):
        self._log = log_callback 
        self.model = model
    async def transcribe(self, audio_filepath: Path) -> AsyncGenerator[str, None]:
        if not WHISPER_IS_READY: yield ""; return
        try:
            audio_input = await asyncio.to_thread(_apply_silero_vad, audio_filepath, self._log)
            if audio_input is None: yield "[NO SPEECH DETECTED]"; return
            result = await asyncio.to_thread(self.model.transcribe, audio_input, language="vi", fp16=USE_FP16)
            yield result.get("text", "").strip()
        except Exception as e:
            self._log(f"‚ùå [ASR] L·ªñI WHISPER: {e}", "red")
            yield "" 

# ==================== D·ªäCH V·ª§ UPLOAD AUDIO ====================

async def _upload_audio_to_internal_api(file_path: Path, session_id: str, log_callback: Callable, api_key: str = INTERNAL_API_KEY):
    """Gi·∫£ l·∫≠p/th·ª±c hi·ªán upload file audio l√™n API n·ªôi b·ªô."""
    if str(INTERNAL_UPLOAD_URL).startswith("http://internal.company.api"):
        log_callback("‚ö†Ô∏è [UPLOAD] B·ªè qua upload: URL v·∫´n l√† placeholder.", "orange")
        return False
        
    try:
        log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] üì§ [UPLOAD] B·∫Øt ƒë·∫ßu upload file: {file_path.name}...", "yellow")
        async with httpx.AsyncClient(verify=False, timeout=30.0) as client: 
            with open(file_path, 'rb') as f:
                files = {'file': (file_path.name, f, 'audio/wav')}
                headers = {'X-API-Key': api_key, 'X-Session-ID': session_id} 
                response = await client.post(INTERNAL_UPLOAD_URL, files=files, headers=headers)
                response.raise_for_status() 
                log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] ‚úÖ [UPLOAD] Upload th√†nh c√¥ng!", "green")
                return True
    except Exception as e:
        log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [UPLOAD] L·ªñI UPLOAD: {e}", "red")
        return False

# ==================== D·ªäCH V·ª§ TTS (Mock Streaming) ====================

class TTSServiceMock:
    """Mock TTS t·∫°o chunk base64 ƒë·ªÉ streaming."""
    def __init__(self, log_callback: Callable): self._log = log_callback
    async def synthesize_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS MOCK] B·∫Øt ƒë·∫ßu t·ªïng h·ª£p √¢m thanh...", "magenta")
        # Gi·∫£ l·∫≠p chunk audio 10ms (16000 sample/s * 2 bytes/sample * 1 k√™nh * 0.01s = 320 bytes)
        mock_chunk_size = 320 
        num_chunks = max(30, min(100, int(len(text) * 0.5) + 10)) 
        
        for _ in range(num_chunks):
            # Gi·∫£ l·∫≠p Base64 encoded chunk
            mock_chunk = base64.b64encode(os.urandom(mock_chunk_size)) 
            yield mock_chunk 
            await asyncio.sleep(0.005) # Gi·∫£ l·∫≠p ƒë·ªô tr·ªÖ streaming
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS MOCK] K·∫øt th√∫c lu·ªìng audio TTS.", "magenta")

# ==================== L·ªöP X·ª¨ L√ù RTC T√çCH H·ª¢P M·ªöI (ƒê√£ s·ª≠a ƒë·ªïi) ====================

class RTCStreamProcessor:
    
    def __init__(self, log_callback: Optional[Callable] = None):
        # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng _log
        self._log = log_callback if log_callback else _log_colored
        self._asr_client = ASRServiceWhisper(self._log, WHISPER_MODEL) if WHISPER_IS_READY else type('ASRMock', (object,), {'transcribe': lambda self, fp: (yield "Transcript gi·∫£ l·∫≠p.")})()
        self._tts_client = TTSServiceMock(self._log)
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y c√°c t√°c v·ª• ƒë·ªìng b·ªô (DM)
        self._executor = ThreadPoolExecutor(max_workers=1)
    
    async def handle_rtc_session(self, 
                                 record_file: Path,
                                 session_id: str,
                                 api_key: str) \
                                 -> AsyncGenerator[Tuple[bool, Any], None]:
        
        self._log(f"‚ñ∂Ô∏è [RTC] B·∫Øt ƒë·∫ßu phi√™n x·ª≠ l√Ω ASR/NLU. Session ID: {session_id}.", "cyan") 
        full_transcript = ""
        response_text = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
        
        try: 
            # KH·ªûI T·∫†O DIALOG MANAGER V·ªöI API KEY
            dm_instance = DialogManager(log_callback=self._log, mode="RTC", api_key=api_key) 
            
            yield (False, {"type": "generator_init", "user_text": "", "bot_text": ""}) 
            
            # 1. UPLOAD AUDIO (B·∫•t ƒë·ªìng b·ªô)
            await _upload_audio_to_internal_api(record_file, session_id, self._log, api_key)
            
            # 2. [ASR Engine] (B·∫•t ƒë·ªìng b·ªô)
            asr_stream = self._asr_client.transcribe(record_file)
            async for partial_text in asr_stream:
                 if partial_text: full_transcript = partial_text
                     
            dm_input_asr = full_transcript.strip() if full_transcript.strip() and partial_text != "[NO SPEECH DETECTED]" else "[NO SPEECH DETECTED]"
            
            # 3-5. [Dialog Manager] (ƒê·ªìng b·ªô)
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM/NLU] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω DialogManager...", "yellow")
            
            # S·ª¨A L·ªñI 1: Thay keyword argument th√†nh positional argument
            dm_result = await asyncio.get_event_loop().run_in_executor(
                 self._executor,
                 dm_instance.process_audio_file, 
                 str(record_file), 
                 dm_input_asr # <--- POSITIONAL ARGUMENT
            )
            response_text = dm_result.get("response_text", response_text)

            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM] Ho√†n t·∫•t. Response: '{response_text[:50]}...'", "green")

            yield (False, {"user_text": full_transcript.strip(), "bot_text": response_text})

            # 6. [TTS Engine] -> [Speaker Output] (Stream)
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS] B·∫Øt ƒë·∫ßu streaming audio ph·∫£n h·ªìi...", "magenta")
            tts_audio_stream = self._tts_client.synthesize_stream(response_text)
            async for audio_chunk in tts_audio_stream:
                yield (True, audio_chunk)
        
        except asyncio.CancelledError:
            raise
        except Exception as e:
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [RTC] L·ªñI X·ª¨ L√ù CHUNG: {e}", "red")
            # S·ª¨A L·ªñI 2: self.log -> self._log
            self._log(traceback.format_exc(), "red") 
            yield (False, {"type": "error", "user_text": full_transcript.strip(), "bot_text": f"L·ªói h·ªá th·ªëng: {e}"})
        finally: 
             self._log(f"[{_dt.now().strftime('%H:%M:%S')}] [RTC] K·∫øt th√∫c x·ª≠ l√Ω RTC.", "cyan")