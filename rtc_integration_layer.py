# rtc_integration_layer.py - ƒê√É FIX L·ªñI BLOCKING CHO VAD/ASR/DM
import asyncio
import os
from pathlib import Path
from typing import AsyncGenerator, Callable, Optional, Tuple, Any
from datetime import datetime as _dt
import numpy as np 
import torch 
import traceback 
import time 
import wave 

# --- SAFE IMPORTS (DIALOG MANAGER) ---
try:
    from dialog_manager import DialogManager 
except ImportError:
    class DialogManager:
        def __init__(self, *args, **kwargs): pass
        def process_audio_file(self, record_file, user_input_asr): 
            # Gi·∫£ l·∫≠p h√†m ƒë·ªìng b·ªô
            return {"response_text": f"L·ªñI DM: Kh√¥ng t√¨m th·∫•y DialogManager. ASR: {user_input_asr}", "response_audio_path": None, "user_input_asr": user_input_asr}

# --- H·∫±ng s·ªë ---
try:
    from config_db import WHISPER_MODEL_NAME, SAMPLE_RATE
except ImportError:
    WHISPER_MODEL_NAME = "tiny" 
    SAMPLE_RATE = 16000 

RECORDING_DIR = Path("rtc_recordings"); RECORDING_DIR.mkdir(exist_ok=True) 

# ----------------------------------------------------------------------
# VAD/ASR IMPORTS
# ----------------------------------------------------------------------
WHISPER_IS_READY = False
try:
    import whisper
    import torchaudio 
        
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_FP16 = (DEVICE == "cuda") 
    
    print(f"‚úÖ [ASR] Thi·∫øt b·ªã ƒë∆∞·ª£c ch·ªçn: {DEVICE}. Whisper model: '{WHISPER_MODEL_NAME}'. FP16: {USE_FP16}", flush=True)

    VAD_MODEL, VAD_UTILS = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    VAD_MODEL = VAD_MODEL.to(DEVICE)
    
    (get_speech_timestamps, save_audio, read_audio, VAD_collect_chunks, *vad_extra_utils) = VAD_UTILS 
    print("‚úÖ [VAD] Silero VAD ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.", flush=True)

    WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME, device=DEVICE) 
    WHISPER_IS_READY = True
except Exception as e:
    DEVICE = "cpu"
    WHISPER_MODEL = None
    print(f"‚ùå [ASR] L·ªói t·∫£i Whisper/VAD: {e}. S·ª≠ d·ª•ng Mock. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t PyTorch/Whisper/torchaudio.", flush=True)


# ==================== VAD HELPER (L·ªçc T·∫°p √Çm) ====================

def _apply_silero_vad(audio_filepath: Path, log_callback: Callable) -> Optional[np.ndarray]:
    """S·ª≠ d·ª•ng Silero VAD ƒë·ªÉ ph√°t hi·ªán v√† c·∫Øt b·ªè c√°c kho·∫£ng l·∫∑ng."""
    if not WHISPER_IS_READY: return None
        
    try:
        audio_tensor = read_audio(str(audio_filepath), sampling_rate=SAMPLE_RATE)
        
        speech_timestamps = get_speech_timestamps(
            audio_tensor.to(DEVICE), 
            VAD_MODEL, 
            sampling_rate=SAMPLE_RATE, 
            threshold=0.3 
        )
        if not speech_timestamps:
            log_callback("‚ö†Ô∏è [VAD] Kh√¥ng t√¨m th·∫•y ho·∫°t ƒë·ªông gi·ªçng n√≥i (speech) trong file.", color="orange")
            return None 

        speech_audio_tensor = VAD_collect_chunks(speech_timestamps, audio_tensor)
        speech_audio_numpy = speech_audio_tensor.cpu().numpy()
        
        original_duration = len(audio_tensor) / SAMPLE_RATE
        filtered_duration = len(speech_audio_numpy) / SAMPLE_RATE
        
        log_callback(f"‚úÖ [VAD] L·ªçc th√†nh c√¥ng. G·ªëc: {original_duration:.2f}s -> VAD: {filtered_duration:.2f}s.", color="blue")
        
        return speech_audio_numpy
    
    except Exception as e:
        log_callback(f"‚ùå [VAD] L·ªñI khi √°p d·ª•ng Silero VAD: {e}. Fallback v·ªÅ audio g·ªëc.", color="red")
        return whisper.load_audio(str(audio_filepath))


# ==================== C√ÅC D·ªäCH V·ª§ ASR ====================

class ASRServiceWhisper:
    def __init__(self, log_callback: Callable, model):
        self._log = log_callback 
        self.model = model
        
    async def transcribe(self, audio_filepath: Path) -> AsyncGenerator[str, None]:
        if not WHISPER_IS_READY: yield ""; return
            
        try:
            start_time = time.time()
            
            # FIX: G·ªåI VAD B·∫∞NG asyncio.to_thread ƒë·ªÉ tr√°nh ch·∫∑n lu·ªìng ch√≠nh
            audio_input = await asyncio.to_thread(
                _apply_silero_vad, audio_filepath, self._log
            )
            
            if audio_input is None or (isinstance(audio_input, np.ndarray) and audio_input.size == 0):
                self._log("‚ö†Ô∏è [ASR] Kh√¥ng c√≥ d·ªØ li·ªáu audio ƒë·ªÉ x·ª≠ l√Ω.", color="orange")
                yield ""
                return
            
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéôÔ∏è [ASR] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Whisper tr√™n {DEVICE}...", color="blue")

            # CH·∫†Y WHISPER TRONG THREADPOOL
            result = await asyncio.to_thread(
                self.model.transcribe, 
                audio_input, 
                device=DEVICE, 
                language="vi", 
                fp16=USE_FP16 
            )
            final_transcript = result.get("text", "").strip()
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] ‚úÖ [ASR] Ho√†n th√†nh. Transcript: '{final_transcript}'. Th·ªùi gian: {processing_time:.2f}s", color="blue")
            
            yield final_transcript
            
        except asyncio.CancelledError:
             raise
        except Exception as e:
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [ASR] L·ªñI WHISPER: {e}", color="red")
            self._log(traceback.format_exc(), color="red")
            yield "" 

class TTSServiceMock:
    def __init__(self, log_callback: Callable): self._log = log_callback
    async def synthesize_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS MOCK] B·∫Øt ƒë·∫ßu t·ªïng h·ª£p √¢m thanh...", color="magenta")
        mock_chunk_size = 320 
        mock_chunk = os.urandom(mock_chunk_size) 
        num_chunks = max(30, min(100, int(len(text) * 0.5) + 10)) 
        for _ in range(num_chunks):
            yield mock_chunk
            await asyncio.sleep(0.005) 
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS MOCK] K·∫øt th√∫c lu·ªìng audio TTS.", color="magenta")

# ==================== L·ªöP X·ª¨ L√ù RTC T√çCH H·ª¢P ====================

class RTCStreamProcessor:
    
    def __init__(self, log_callback: Optional[Callable] = None):
        def default_log(message, color=None):
            print(f"[{_dt.now().strftime('%H:%M:%S')}] [LOG] {message}", flush=True)

        self._log = log_callback if log_callback else default_log

        if WHISPER_IS_READY:
            self._asr_client = ASRServiceWhisper(self._log, WHISPER_MODEL)
            self._asr_mode = "WHISPER"
        else:
            class ASRServiceMock:
                def __init__(self, log_callback): self._log = log_callback
                async def transcribe(self, audio_filepath: Path): yield "Transcript gi·∫£ l·∫≠p."
            self._asr_client = ASRServiceMock(self._log)
            self._asr_mode = "MOCK"


        self._dm = DialogManager(log_callback=self._log, mode="RTC") 
        self._tts_client = TTSServiceMock(self._log)

    async def handle_rtc_session(self, 
                                 record_file: Path,
                                 session_id: str) \
                                 -> AsyncGenerator[Tuple[bool, Any], None]:
        
        self._log(f"‚ñ∂Ô∏è [RTC] B·∫Øt ƒë·∫ßu phi√™n x·ª≠ l√Ω ASR/NLU. Session ID: {session_id}. File: {record_file.name}", color="cyan") 
        
        full_transcript = ""
        response_text = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
        
        try: 
            # 1. X·ª≠ l√Ω ASR (Bao g·ªìm VAD)
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéôÔ∏è [ASR] B·∫Øt ƒë·∫ßu g·ªçi ASR Service...", color="yellow")
            asr_stream = self._asr_client.transcribe(record_file)
            async for partial_text in asr_stream:
                 if partial_text:
                     full_transcript = partial_text
                     
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéôÔ∏è [ASR] Transcript nh·∫≠n ƒë∆∞·ª£c: '{full_transcript[:50]}...'", color="green")

            # 2. NLU/Response Logic - T√çCH H·ª¢P DIALOG MANAGER
            dm_input_asr = full_transcript.strip() if full_transcript.strip() else "[NO SPEECH DETECTED]"
            
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω DialogManager (Chuy·ªÉn sang lu·ªìng ph·ª•)...", color="yellow")
            
            # ‚úÖ FIX: G·ªåI H√ÄM ƒê·ªíNG B·ªò C·ª¶A DIALOG MANAGER TRONG THREADPOOL
            dm_result = await asyncio.to_thread(
                self._dm.process_audio_file, 
                str(record_file), 
                user_input_asr=dm_input_asr
            )
                
            response_text = dm_result.get("response_text", response_text)
            
            if dm_input_asr == "[NO SPEECH DETECTED]":
                 response_text = "Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i kh√¥ng?"

            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM] Ho√†n t·∫•t. Response: '{response_text[:50]}...' (mode: {self._asr_mode})", color="green")


            yield (False, {"user_text": full_transcript, "bot_text": response_text})

            # 3. TTS v√† Tr·∫£ v·ªÅ Lu·ªìng Audio
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS] B·∫Øt ƒë·∫ßu streaming audio ph·∫£n h·ªìi...", color="magenta")
            tts_audio_stream = self._tts_client.synthesize_stream(response_text)
            async for audio_chunk in tts_audio_stream:
                yield (True, audio_chunk)
        
        except asyncio.CancelledError:
            raise
        except Exception as e:
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [RTC] L·ªñI X·ª¨ L√ù CHUNG: {e}", color="red")
            self._log(traceback.format_exc(), color="red")
        finally: 
             self._log(f"[{_dt.now().strftime('%H:%M:%S')}] [RTC] K·∫øt th√∫c x·ª≠ l√Ω RTC.", color="cyan")